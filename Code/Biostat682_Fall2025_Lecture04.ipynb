{"cells":[{"cell_type":"markdown","id":"cbc67167","metadata":{"id":"cbc67167"},"source":["\n","# SBP Example with Jeffreys Prior (Normal mean & variance unknown)\n","\n","**Goal.** Use Jeffreys prior for a Normal model with unknown mean and variance to answer:\n","\n","1) Posterior probability that the population mean SBP exceeds 130 mmHg.  \n","2) Posterior *predictive* probability that a new patient’s SBP exceeds 130 mmHg.\n","\n","**Data (n = 10):**\n","$$\n","y = (128,\\;132,\\;121,\\;135,\\;126,\\;130,\\;129,\\;138,\\;125,\\;131)\n","$$\n","\n","We'll assume $ y_i \\mid \\mu,\\sigma^2 \\stackrel{iid}{\\sim} \\mathcal{N}(\\mu,\\sigma^2) $.  \n","Jeffreys prior for $(\\mu,\\sigma^2)$ is $ \\pi(\\mu,\\sigma^2) \\propto 1/\\sigma^2 $.\n"]},{"cell_type":"markdown","id":"5cbe48b7","metadata":{"id":"5cbe48b7"},"source":["\n","## Jeffreys Prior Derivation (Precision Parameterization)\n","\n","We work with the precision parameterization:\n","$$\n","y_i \\mid \\mu,\\tau^2 \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(\\mu,\\;\\tau^{-2}),\n","\\qquad \\tau^2 = \\sigma^{-2}, \\quad i=1,\\ldots,10.\n","$$\n","\n","**Interpretation.**\n","- $\\mu$: population mean SBP (scientific target)  \n","- $\\tau^2$: precision (inverse variance); larger $\\tau^2$ = less between-patient variability  \n","- $\\sigma^2 = 1/\\tau^2$: variance (often treated as a nuisance parameter)\n","\n","**Log-likelihood:**\n","$$\n","\\ell(\\mu,\\tau^2) = \\frac{n}{2}\\log \\tau^2 - \\frac{\\tau^2}{2}\\sum_{i=1}^n (y_i-\\mu)^2 + \\text{const.}\n","$$\n","\n","**Scores and Fisher information.**\n","$$\n","\\frac{\\partial \\ell}{\\partial \\mu} = \\tau^2 \\sum_{i=1}^n (y_i-\\mu),\n","\\quad\n","\\frac{\\partial^2 \\ell}{\\partial \\mu^2} = - n \\tau^2\n","\\;\\Rightarrow\\;\n","I_{\\mu\\mu}(\\mu,\\tau^2)= n \\tau^2.\n","$$\n","\n","$$\n","\\frac{\\partial \\ell}{\\partial \\tau^2} = \\frac{n}{2\\tau^2} - \\frac{1}{2}\\sum_{i=1}^n (y_i-\\mu)^2,\n","\\quad\n","\\frac{\\partial^2 \\ell}{\\partial (\\tau^2)^2} = -\\frac{n}{2(\\tau^2)^2}\n","\\;\\Rightarrow\\;\n","I_{\\tau^2\\tau^2}(\\mu,\\tau^2)= \\frac{n}{2(\\tau^2)^2}.\n","$$\n","\n","$$\n","\\frac{\\partial^2 \\ell}{\\partial \\mu\\,\\partial \\tau^2}\n","= \\sum_{i=1}^n (y_i-\\mu)\n","\\;\\Rightarrow\\;\n","I_{\\mu,\\tau^2}(\\mu,\\tau^2)=0.\n","$$\n","\n","Hence\n","$$\n","I(\\mu,\\tau^2)=\n","\\begin{pmatrix}\n","n\\tau^2 & 0\\$$2pt]\n","0 & \\dfrac{n}{2(\\tau^2)^2}\n","\\end{pmatrix},\n","\\qquad\n","\\det I(\\mu,\\tau^2)=\\frac{n^2}{2}\\cdot \\frac{1}{\\tau^2}.\n","$$\n","\n","**Jeffreys prior:**\n","$$\n","\\pi_J(\\mu,\\tau^2)\\ \\propto\\ \\sqrt{\\det I(\\mu,\\tau^2)}\n","\\ \\propto\\ \\frac{1}{\\sqrt{\\tau^2}}.\n","$$\n","\n","> *Remark.* Many texts instead use the (reference/independence) prior $\\pi(\\mu,\\sigma^2)\\propto 1/\\sigma^2$, which, under the reparameterization $\\tau^2=1/\\sigma^2$, becomes $\\pi(\\mu,\\tau^2)\\propto 1/\\tau^2$. Both choices lead to the familiar Student-$t$ marginal for $\\mu$; the difference only affects the conditional prior on the precision.\n"]},{"cell_type":"markdown","id":"8c7bcfd1","metadata":{"id":"8c7bcfd1"},"source":["\n","## Marginal Posterior Distribution of $\\mu$\n","\n","- Recall: $ \\mu \\mid \\tau^2, y \\sim \\mathcal{N}(\\bar y, (n\\tau^2)^{-1}) $, and $ \\tau^2 \\mid y \\sim \\mathrm{Gamma}(n/2, S/2) $.  \n","- Define $ \\kappa = S \\tau^2/n $. Then $ \\kappa \\mid y \\sim \\mathrm{Gamma}(n/2, n/2) $.  \n","- Note: $(n\\tau^2)^{-1} = \\tfrac{S}{n^2} \\kappa^{-1}$. Thus\n","  $$ \\mu \\mid \\kappa,y \\sim \\mathcal{N}\\!\\left(\\bar y, \\tfrac{S}{n^2}\\kappa^{-1}\\right). $$\n","- By the scale mixture of Normals representation, we obtain:\n","$$\n","\\boxed{\n","\\mu \\mid y \\sim t_{n}\\!\\left(\\bar y,\\; S/n^2\\right)\n","= t_{n}\\!\\left(\\bar y,\\; \\left(1-\\tfrac{1}{n}\\right)\\tfrac{s^2}{n}\\right)\n","}\n","$$\n","where $ S = \\sum_{i=1}^n (y_i - \\bar y)^2 $, and $ s^2 = S/(n-1) $.\n"]},{"cell_type":"markdown","id":"416cc600","metadata":{"id":"416cc600"},"source":["\n","## Posterior Predictive Distribution\n","\n","Let $ \\tilde y \\mid \\mu,\\tau^2 \\sim \\mathcal{N}(\\mu, \\tau^{-2}) $.  \n","\n","The joint posterior is  \n","$$\n","\\mu \\mid \\tau^2,y \\sim \\mathcal{N}(\\bar y, (n\\tau^2)^{-1}),\n","\\qquad \\tau^2 \\mid y \\sim \\mathrm{Gamma}(n/2, S/2).\n","$$\n","\n","1. Integrate out $\\mu$ given $\\tau^2$:  \n","   $$\n","   \\tilde y \\mid \\tau^2 \\sim \\mathcal{N}\\!\\left(\\bar y,\\; (1+n^{-1})(\\tau^2)^{-1}\\right).\n","   $$\n","\n","2. Then integrate out $\\tau^2$:  \n","   $$\n","   \\boxed{\n","   \\tilde y \\mid y \\sim t_{n}\\!\\left(\\bar y,\\; \\left(1+\\tfrac{1}{n}\\right)\\tfrac{S}{n}\\right)\n","   = t_{n}\\!\\left(\\bar y,\\; \\left(1-\\tfrac{1}{n^2}\\right) s^2\\right)\n","   }\n","   $$\n"]},{"cell_type":"code","execution_count":1,"id":"e9602988","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9602988","executionInfo":{"status":"ok","timestamp":1758554816313,"user_tz":240,"elapsed":21,"user":{"displayName":"Jian Kang","userId":"16525353915062406154"}},"outputId":"5caaa749-b5b0-4da8-fc4d-73c72546b750"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, np.float64(129.5), np.float64(24.27777777777778), np.float64(218.5))"]},"metadata":{},"execution_count":1}],"source":["\n","import numpy as np\n","\n","y = np.array([128, 132, 121, 135, 126, 130, 129, 138, 125, 131], dtype=float)\n","n = len(y)\n","ybar = y.mean()\n","S = ((y - ybar)**2).sum()          # Sum of squares about the mean\n","s2 = S/(n-1)                       # Unbiased sample variance\n","\n","n, ybar, s2, S\n"]},{"cell_type":"markdown","id":"7c3c42cb","metadata":{"id":"7c3c42cb"},"source":["\n","## Marginal Posterior of $\\mu$ and Posterior Predictive (df $= n$) under $\\pi(\\mu,\\tau^2)\\propto (\\tau^2)^{-1/2}$\n","\n","From the precision-parameterized derivation:\n","\n","- $ \\mu \\mid \\tau^2, y \\sim \\mathcal{N}\\!\\big(\\bar y,\\; (n\\tau^2)^{-1}\\big) $,  \n","  $ \\tau^2 \\mid y \\sim \\mathrm{Gamma}\\!\\left(\\frac{n}{2}, \\frac{S}{2}\\right) $ (shape–rate).\n","\n","Let $ \\kappa = S\\tau^2/n $. Then $ \\kappa \\mid y \\sim \\mathrm{Gamma}\\!\\left(\\frac{n}{2}, \\frac{n}{2}\\right) $.  \n","Also $ (n\\tau^2)^{-1} = \\frac{S}{n^2}\\kappa^{-1} $, hence\n","$$\n","\\mu \\mid \\kappa, y \\sim \\mathcal{N}\\!\\left(\\bar y, \\frac{S}{n^2}\\kappa^{-1}\\right).\n","$$\n","\n","By the Normal–Gamma scale-mixture representation,\n","$$\n","\\boxed{\\;\\mu \\mid y \\sim t_{n}\\!\\left(\\bar y,\\; \\frac{S}{n^2}\\right) = t_{n}\\!\\left(\\bar y,\\; \\left(1-\\frac{1}{n}\\right)\\frac{s^2}{n}\\right)\\;}\n","$$\n","with $ S=\\sum_{i=1}^n (y_i - \\bar y)^2 $ and $ s^2=S/(n-1) $.\n","\n","For the posterior predictive, integrating out $\\mu$ and then $\\tau^2$ yields\n","$$\n","\\boxed{\\;\\tilde y \\mid y \\sim t_{n}\\!\\left(\\bar y,\\; \\left(1+\\frac{1}{n}\\right)\\frac{S}{n}\\right)\n","= t_{n}\\!\\left(\\bar y,\\; \\left(1 - \\frac{1}{n^2}\\right) s^2\\right)\\;}\n","$$\n","where $t_{\\nu}(\\text{location}, \\text{scale})$ is Student-$t$ with df $\\nu$, location shift, and scale.\n"]},{"cell_type":"code","execution_count":2,"id":"b5036194","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5036194","executionInfo":{"status":"ok","timestamp":1758554889823,"user_tz":240,"elapsed":664,"user":{"displayName":"Jian Kang","userId":"16525353915062406154"}},"outputId":"9823be77-6061-4763-806d-202093b9b921"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(np.float64(0.3710826258510036), np.float64(0.46039124016169886))"]},"metadata":{},"execution_count":2}],"source":["\n","from scipy.stats import t, gamma\n","import numpy as np\n","\n","# Using df = n and the scales derived above\n","nu_alt = n  # degrees of freedom\n","threshold = 130.0\n","\n","# Posterior for mu: df=n, scale = sqrt(S/n^2)\n","scale_mu_alt = np.sqrt(S/(n**2))\n","post_mu_prob_alt = 1 - t.cdf((threshold - ybar)/scale_mu_alt, df=nu_alt)\n","\n","# Posterior predictive: df=n, scale = sqrt((1 + 1/n) * S / n)\n","scale_pred_alt = np.sqrt((1 + 1/n) * S / n)\n","post_pred_prob_alt = 1 - t.cdf((threshold - ybar)/scale_pred_alt, df=nu_alt)\n","\n","post_mu_prob_alt, post_pred_prob_alt\n"]},{"cell_type":"markdown","id":"c107ac08","metadata":{"id":"c107ac08"},"source":["\n","### Monte Carlo check for the $t_n$ results\n","\n","Under $\\pi(\\mu,\\tau^2)\\propto (\\tau^2)^{-1/2}$:  \n","- Sample $ \\tau^2 \\mid y \\sim \\mathrm{Gamma}\\!\\left(\\frac{n}{2}, \\frac{S}{2}\\right) $ (shape–rate).  \n","- Then $ \\mu \\mid \\tau^2, y \\sim \\mathcal{N}\\!\\big(\\bar y,\\; (n\\tau^2)^{-1}\\big) $.  \n","- Predictive $ \\tilde y \\mid \\mu,\\tau^2 \\sim \\mathcal{N}(\\mu,\\tau^{-2}) $.\n"]},{"cell_type":"code","execution_count":3,"id":"4ca61f8c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ca61f8c","executionInfo":{"status":"ok","timestamp":1758554924210,"user_tz":240,"elapsed":75,"user":{"displayName":"Jian Kang","userId":"16525353915062406154"}},"outputId":"ad6ded5a-8524-4468-b4ac-ace4560bd6a7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(np.float64(0.3710826258510036),\n"," np.float64(0.370792),\n"," np.float64(0.46039124016169886),\n"," np.float64(0.461002))"]},"metadata":{},"execution_count":3}],"source":["\n","rng = np.random.default_rng(2025)\n","B = 500_000\n","\n","# tau^2 | y ~ Gamma(shape=n/2, rate=S/2)\n","shape = n/2\n","rate = S/2\n","tau2 = rng.gamma(shape=shape, scale=1/rate, size=B)\n","\n","# mu | tau^2, y\n","mu_alt = rng.normal(loc=ybar, scale=np.sqrt(1/(n*tau2)))\n","\n","# predictive\n","ynew_alt = rng.normal(loc=mu_alt, scale=np.sqrt(1/tau2))\n","\n","mc_mu_alt = (mu_alt > threshold).mean()\n","mc_pred_alt = (ynew_alt > threshold).mean()\n","\n","(post_mu_prob_alt, mc_mu_alt, post_pred_prob_alt, mc_pred_alt)\n"]},{"cell_type":"code","execution_count":4,"id":"9ca67c91","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ca67c91","executionInfo":{"status":"ok","timestamp":1758554931061,"user_tz":240,"elapsed":5,"user":{"displayName":"Jian Kang","userId":"16525353915062406154"}},"outputId":"6c5b2de4-fa2d-437a-bd52-f165ac442567"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Using df = n (Jeffreys prior in precision form π(μ,τ^2) ∝ (τ^2)^(-1/2)) ===\n","n = 10, ȳ = 129.500, s^2 = 24.278, S = 218.500\n","P(μ > 130 | y) [analytic]     = 0.3711\n","P(μ > 130 | y) [MC]           = 0.3708\n","P(y_new > 130 | y) [analytic] = 0.4604\n","P(y_new > 130 | y) [MC]       = 0.4610\n","\n","For reference, earlier cells used the common reference prior π(μ,σ^2) ∝ 1/σ^2, which yields df = n-1.\n","Both produce Student-t forms; the df and scales differ slightly due to the prior choice.\n"]}],"source":["\n","print(\"=== Using df = n (Jeffreys prior in precision form π(μ,τ^2) ∝ (τ^2)^(-1/2)) ===\")\n","print(f\"n = {n}, ȳ = {ybar:.3f}, s^2 = {s2:.3f}, S = {S:.3f}\")\n","print(f\"P(μ > 130 | y) [analytic]     = {post_mu_prob_alt:.4f}\")\n","print(f\"P(μ > 130 | y) [MC]           = {mc_mu_alt:.4f}\")\n","print(f\"P(y_new > 130 | y) [analytic] = {post_pred_prob_alt:.4f}\")\n","print(f\"P(y_new > 130 | y) [MC]       = {mc_pred_alt:.4f}\")\n","\n","print(\"\\nFor reference, earlier cells used the common reference prior π(μ,σ^2) ∝ 1/σ^2, which yields df = n-1.\")\n","print(\"Both produce Student-t forms; the df and scales differ slightly due to the prior choice.\")\n"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}