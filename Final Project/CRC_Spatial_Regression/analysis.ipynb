{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Bayesian Analysis of Colorectal Cancer Tissue Imaging Data\n",
    "\n",
    "**BIOSTAT 682 Final Project**  \n",
    "**Author:** Santosh Desai  \n",
    "**Date:** December 2025  \n",
    "**Institution:** University of Michigan\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook presents a comprehensive Bayesian analysis of multiplexed tissue imaging data from colorectal cancer (CRC) samples. The analysis employs multiple Bayesian regression models to understand the relationship between PD-1 (Programmed Death-1) checkpoint marker expression and other immune cell markers, while accounting for spatial structure in the tumor microenvironment.\n",
    "\n",
    "### Research Question\n",
    "\n",
    "How do immune checkpoint markers (specifically PD-1) relate to other immune cell markers and spatial location in the tumor microenvironment? We address this through:\n",
    "\n",
    "1. **Multiple regression models** with different prior specifications\n",
    "2. **Variable selection** using spike-and-slab priors\n",
    "3. **Non-linear relationships** via Bayesian Neural Networks (BNNs)\n",
    "4. **Spatial effects** to account for tissue structure\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The dataset (`CRC_data_55A.csv`) contains:\n",
    "- **2,874 cells** from a colorectal cancer tissue sample\n",
    "- **Spatial coordinates** (cx, cy) for each cell's location in the tissue\n",
    "- **20 protein markers** including:\n",
    "  - Immune checkpoint markers: PD-1, LAG-3, VISTA, ICOS\n",
    "  - T cell markers: CD2, CD5, CD25\n",
    "  - Macrophage markers: CD68, CD11b\n",
    "  - Dendritic cell markers: CD21\n",
    "  - NK cell markers: CD56\n",
    "  - Signaling markers: beta-catenin, EGFR, CD44\n",
    "  - Other immune markers: GATA3, CD38, Podoplanin, IDO-1\n",
    "\n",
    "### Analysis Workflow\n",
    "\n",
    "This notebook follows a rigorous Bayesian workflow:\n",
    "\n",
    "1. **Data Loading and Preprocessing**: Load, validate, and standardize data\n",
    "2. **Exploratory Data Analysis**: Comprehensive visualization and summary statistics\n",
    "3. **Model 1**: Bayesian linear regression with noninformative priors\n",
    "4. **Model 2**: Bayesian linear regression with hierarchical priors\n",
    "5. **Model 3**: Spike-and-slab variable selection\n",
    "6. **Model 4**: Bayesian Neural Network (BNN) with spike-and-slab priors\n",
    "7. **Model 5**: Spatial regression model with coordinate effects\n",
    "8. **Model Comparison**: PSIS-LOO, WAIC, and predictive performance\n",
    "9. **Posterior Analysis**: Credible intervals, posterior predictive checks\n",
    "10. **Convergence Diagnostics**: R-hat, ESS, MCSE, Geweke diagnostics\n",
    "\n",
    "All models are fit using MCMC (NUTS sampler) with comprehensive convergence diagnostics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We begin by importing all necessary libraries and setting up the computational environment for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymc>=5.10.0 arviz>=0.17.0 seaborn>=0.12.0 scikit-learn>=1.3.0\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", message=\".*install.*ipywidgets.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm, t as student_t\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set professional plotting style\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"husl\")\n",
    "plt.rcParams.update({\n",
    "    \"figure.autolayout\": True,\n",
    "    \"font.size\": 11,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"figure.titlesize\": 16,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": [\"Arial\", \"DejaVu Sans\", \"Liberation Sans\", \"Helvetica\", \"sans-serif\"]\n",
    "})\n",
    "\n",
    "# Reproducibility\n",
    "RNG_SEED: int = 42\n",
    "rng: np.random.Generator = np.random.default_rng(RNG_SEED)\n",
    "\n",
    "# Display options\n",
    "pd.options.display.float_format = \"{:0.3f}\".format\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE BAYESIAN ANALYSIS OF CRC TISSUE IMAGING DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nPyMC version: {pm.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Random seed: {RNG_SEED}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "We load the CRC tissue imaging data and perform comprehensive preprocessing including:\n",
    "- Data validation and quality checks\n",
    "- Standardization of predictors and response\n",
    "- Train/test split for predictive evaluation\n",
    "- Creation of standardized coordinate features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Standardizer:\n",
    "    \"\"\"Standardizer for data preprocessing.\"\"\"\n",
    "    mean: pd.Series\n",
    "    std: pd.Series\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform data using fitted mean and std.\"\"\"\n",
    "        z = (X - self.mean) / self.std.replace(0, 1.0)\n",
    "        return z\n",
    "    \n",
    "    @classmethod\n",
    "    def fit(cls, X: pd.DataFrame) -> \"Standardizer\":\n",
    "        \"\"\"Fit standardizer to data.\"\"\"\n",
    "        return cls(mean=X.mean(), std=X.std(ddof=1))\n",
    "    \n",
    "    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Inverse transform to original scale.\"\"\"\n",
    "        return X * self.std + self.mean\n",
    "\n",
    "\n",
    "def load_crc_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CRC tissue imaging data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    path : str\n",
    "        Path to CSV file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Loaded data frame\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"Loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load data\n",
    "data_path = \"data/CRC_data_55A.csv\"\n",
    "df_raw = load_crc_data(data_path)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df_raw.head())\n",
    "\n",
    "# Display column names\n",
    "print(f\"\\nColumn names ({len(df_raw.columns)} total):\")\n",
    "print(df_raw.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spatial coordinates\n",
    "coords = df_raw[['cx', 'cy']].values\n",
    "print(f\"Spatial coordinates shape: {coords.shape}\")\n",
    "print(f\"Coordinate ranges:\")\n",
    "print(f\"  cx: [{coords[:, 0].min():.3f}, {coords[:, 0].max():.3f}]\")\n",
    "print(f\"  cy: [{coords[:, 1].min():.3f}, {coords[:, 1].max():.3f}]\")\n",
    "\n",
    "# Extract all marker columns (exclude coordinates)\n",
    "marker_cols = [col for col in df_raw.columns if col not in ['cx', 'cy']]\n",
    "markers_df = df_raw[marker_cols]\n",
    "\n",
    "print(f\"\\nTotal markers: {len(marker_cols)}\")\n",
    "\n",
    "# Identify response variable (PD-1)\n",
    "pd1_col = [col for col in marker_cols if 'PD-1' in col]\n",
    "if not pd1_col:\n",
    "    raise ValueError(\"PD-1 column not found!\")\n",
    "pd1_col = pd1_col[0]\n",
    "print(f\"\\nResponse variable: {pd1_col}\")\n",
    "\n",
    "y_raw = df_raw[pd1_col].values\n",
    "print(f\"\\nResponse statistics (raw scale):\")\n",
    "print(f\"  Mean: {y_raw.mean():.3f}\")\n",
    "print(f\"  Std: {y_raw.std():.3f}\")\n",
    "print(f\"  Min: {y_raw.min():.3f}\")\n",
    "print(f\"  Max: {y_raw.max():.3f}\")\n",
    "print(f\"  Median: {np.median(y_raw):.3f}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing = df_raw.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(f\"\\n⚠ Warning: {missing.sum()} missing values found:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"\\n✓ No missing values detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select predictor markers\n",
    "# Focus on key immune markers for interpretability and model complexity\n",
    "predictor_candidates = [\n",
    "    col for col in marker_cols \n",
    "    if col != pd1_col and any(marker in col for marker in [\n",
    "        'CD2', 'CD5', 'CD25', 'LAG-3', 'VISTA', 'ICOS', \n",
    "        'CD68', 'CD11b', 'CD21', 'GATA3', 'CD56', 'CD38'\n",
    "    ])\n",
    "]\n",
    "\n",
    "print(f\"Candidate predictors: {len(predictor_candidates)}\")\n",
    "\n",
    "# Select top predictors based on correlation with PD-1\n",
    "correlations = markers_df[predictor_candidates].corrwith(markers_df[pd1_col]).abs().sort_values(ascending=False)\n",
    "selected_predictors = correlations.head(10).index.tolist()  # Use top 10 for comprehensive analysis\n",
    "\n",
    "print(f\"\\nSelected {len(selected_predictors)} predictors (top correlations with PD-1):\")\n",
    "for i, (pred, corr) in enumerate(zip(selected_predictors, correlations[selected_predictors]), 1):\n",
    "    print(f\"  {i:2d}. {pred.split(':')[0]:30s} (|r| = {corr:.3f})\")\n",
    "\n",
    "# Extract predictor matrix\n",
    "X_raw = markers_df[selected_predictors].values\n",
    "n, p = X_raw.shape\n",
    "\n",
    "print(f\"\\nData dimensions:\")\n",
    "print(f\"  Sample size (n): {n}\")\n",
    "print(f\"  Number of predictors (p): {p}\")\n",
    "print(f\"  Response variable: PD-1 expression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize predictors and response\n",
    "# Standardization improves numerical stability and MCMC mixing\n",
    "# Coefficients are interpreted as change per 1 SD increase in predictor\n",
    "\n",
    "# Standardize predictors\n",
    "X_scaler = Standardizer.fit(pd.DataFrame(X_raw, columns=selected_predictors))\n",
    "X_scaled_df = X_scaler.transform(pd.DataFrame(X_raw, columns=selected_predictors))\n",
    "X_scaled = X_scaled_df.values\n",
    "\n",
    "# Standardize response\n",
    "y_scaler = Standardizer.fit(pd.Series(y_raw, name=pd1_col))\n",
    "y_scaled = y_scaler.transform(pd.Series(y_raw, name=pd1_col)).values\n",
    "\n",
    "# Standardize coordinates\n",
    "coords_scaler = Standardizer.fit(pd.DataFrame(coords, columns=['cx', 'cy']))\n",
    "coords_scaled_df = coords_scaler.transform(pd.DataFrame(coords, columns=['cx', 'cy']))\n",
    "coords_scaled = coords_scaled_df.values\n",
    "\n",
    "print(\"Standardization complete:\")\n",
    "print(f\"  Predictors: mean ≈ {X_scaled.mean(axis=0)[:3]}, std ≈ {X_scaled.std(axis=0, ddof=1)[:3]}\")\n",
    "print(f\"  Response: mean = {y_scaled.mean():.6f}, std = {y_scaled.std(ddof=1):.6f}\")\n",
    "print(f\"  Coordinates: mean ≈ {coords_scaled.mean(axis=0)}, std ≈ {coords_scaled.std(axis=0, ddof=1)}\")\n",
    "\n",
    "# Store scaling parameters for later use\n",
    "X_mean = X_scaler.mean.values\n",
    "X_std = X_scaler.std.values\n",
    "y_mean = y_scaler.mean.values[0]\n",
    "y_std = y_scaler.std.values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split for predictive evaluation\n",
    "# Use 70/30 split to have sufficient data for both training and testing\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(n), \n",
    "    test_size=0.3, \n",
    "    random_state=RNG_SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train = X_scaled[train_idx]\n",
    "X_test = X_scaled[test_idx]\n",
    "y_train = y_scaled[train_idx]\n",
    "y_test = y_scaled[test_idx]\n",
    "coords_train = coords_scaled[train_idx]\n",
    "coords_test = coords_scaled[test_idx]\n",
    "\n",
    "n_train = len(train_idx)\n",
    "n_test = len(test_idx)\n",
    "\n",
    "print(f\"Train/test split:\")\n",
    "print(f\"  Training set: {n_train} observations ({100*n_train/n:.1f}%)\")\n",
    "print(f\"  Test set: {n_test} observations ({100*n_test/n:.1f}%)\")\n",
    "print(f\"  Total: {n} observations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "We perform comprehensive exploratory data analysis to understand:\n",
    "- Distribution of response variable (PD-1 expression)\n",
    "- Spatial distribution of PD-1 in the tissue\n",
    "- Correlations between markers\n",
    "- Relationships between predictors and response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Spatial distribution of PD-1 expression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Spatial scatter plot\n",
    "scatter1 = axes[0].scatter(coords[:, 0], coords[:, 1], c=y_raw, cmap='viridis', \n",
    "                           s=2, alpha=0.6, rasterized=True)\n",
    "axes[0].set_xlabel('X coordinate', fontsize=12)\n",
    "axes[0].set_ylabel('Y coordinate', fontsize=12)\n",
    "axes[0].set_title('Spatial Distribution of PD-1 Expression', fontsize=14, fontweight='bold')\n",
    "cbar1 = plt.colorbar(scatter1, ax=axes[0], label='PD-1 Expression')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of response\n",
    "axes[1].hist(y_raw, bins=60, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[1].axvline(y_raw.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean = {y_raw.mean():.2f}')\n",
    "axes[1].axvline(np.median(y_raw), color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Median = {np.median(y_raw):.2f}')\n",
    "axes[1].set_xlabel('PD-1 Expression', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Distribution of PD-1 Expression', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('report/figures/figure1_spatial_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure 1: Spatial distribution and histogram of PD-1 expression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Correlation heatmap\n",
    "corr_data = markers_df[[pd1_col] + selected_predictors]\n",
    "corr_matrix = corr_data.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "            vmin=-1, vmax=1, annot_kws={'size': 9})\n",
    "plt.title('Correlation Matrix: PD-1 and Selected Predictors', \n",
    "         fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('report/figures/figure2_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure 2: Correlation heatmap showing relationships between PD-1 and predictors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics table\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Mean': markers_df[[pd1_col] + selected_predictors].mean(),\n",
    "    'Std': markers_df[[pd1_col] + selected_predictors].std(),\n",
    "    'Min': markers_df[[pd1_col] + selected_predictors].min(),\n",
    "    'Max': markers_df[[pd1_col] + selected_predictors].max(),\n",
    "    'Median': markers_df[[pd1_col] + selected_predictors].median(),\n",
    "    'Correlation_with_PD1': [1.0] + [correlations[pred] for pred in selected_predictors]\n",
    "})\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"=\" * 100)\n",
    "display(summary_stats)\n",
    "\n",
    "# Save for report table\n",
    "summary_stats.to_csv('report/summary_statistics.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 1: Bayesian Linear Regression with Noninformative Priors\n",
    "\n",
    "We begin with a standard Bayesian linear regression model using noninformative (flat) priors. This serves as a baseline and allows us to compare with more structured priors in subsequent models.\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "**Likelihood:**\n",
    "$$y_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$$\n",
    "\n",
    "where\n",
    "$$\\mu_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}$$\n",
    "\n",
    "**Prior Distributions:**\n",
    "- Intercept: $p(\\beta_0) \\propto 1$ (improper uniform)\n",
    "- Coefficients: $p(\\beta_j) \\propto 1$ for $j = 1, \\ldots, p$ (improper uniform)\n",
    "- Error variance: $p(\\sigma^2) \\propto 1/\\sigma^2$ (Jeffreys prior)\n",
    "\n",
    "This specification yields a conjugate normal-inverse-gamma posterior, but we use MCMC for consistency with other models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODEL 1: Bayesian Linear Regression (Noninformative Priors)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with pm.Model() as model1:\n",
    "    # Noninformative priors (very diffuse)\n",
    "    beta_0 = pm.Normal(\"beta_0\", mu=0.0, sigma=100.0)  # Very diffuse\n",
    "    beta = pm.Normal(\"beta\", mu=0.0, sigma=100.0, shape=p)  # Very diffuse\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=10.0)  # Weakly informative\n",
    "    \n",
    "    # Linear predictor\n",
    "    mu = beta_0 + pm.math.dot(X_train, beta)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_train)\n",
    "\n",
    "print(\"Model 1 built successfully!\")\n",
    "print(f\"  Parameters: {p + 2} (1 intercept + {p} coefficients + 1 error variance)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model 1\n",
    "print(\"\\nSampling from posterior (Model 1)...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "with model1:\n",
    "    trace1 = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=2000,\n",
    "        chains=4,\n",
    "        cores=1,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RNG_SEED,\n",
    "        return_inferencedata=True,\n",
    "        progressbar=True\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Model 1 sampling completed!\")\n",
    "\n",
    "# Convergence diagnostics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 1: Convergence Diagnostics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "rhat1 = az.rhat(trace1)\n",
    "max_rhat1 = max([float(rhat1[var].max()) for var in rhat1.data_vars])\n",
    "print(f\"\\nR-hat (max): {max_rhat1:.4f} (target: < 1.01)\")\n",
    "\n",
    "ess1 = az.ess(trace1)\n",
    "min_ess1 = min([float(ess1[var].min()) for var in ess1.data_vars])\n",
    "print(f\"ESS (min): {min_ess1:.0f} (target: > 400)\")\n",
    "\n",
    "n_div1 = trace1.sample_stats.divergences.sum().item()\n",
    "print(f\"Divergences: {n_div1} (target: 0)\")\n",
    "\n",
    "# Posterior summary\n",
    "summary1 = az.summary(trace1, var_names=['beta_0', 'beta', 'sigma'])\n",
    "print(\"\\nPosterior Summary (Model 1):\")\n",
    "print(\"=\" * 80)\n",
    "display(summary1.head(15))  # Show first 15 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 2: Bayesian Linear Regression with Hierarchical Priors\n",
    "\n",
    "We now fit a Bayesian linear regression with hierarchical (shrinkage) priors. This model regularizes coefficients toward zero, which helps with overfitting and provides better generalization.\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "**Likelihood:**\n",
    "$$y_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$$\n",
    "\n",
    "where\n",
    "$$\\mu_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}$$\n",
    "\n",
    "**Prior Distributions:**\n",
    "- Intercept: $\\beta_0 \\sim \\mathcal{N}(0, 10^2)$\n",
    "- Coefficients: $\\beta_j \\sim \\mathcal{N}(0, \\tau^2)$ for $j = 1, \\ldots, p$\n",
    "- Global scale: $\\tau^2 \\sim \\text{InverseGamma}(2, 1)$\n",
    "- Error variance: $\\sigma^2 \\sim \\text{InverseGamma}(2, 1)$\n",
    "\n",
    "The hierarchical structure allows the data to inform the appropriate level of shrinkage through the global scale parameter $\\tau^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODEL 2: Bayesian Linear Regression (Hierarchical Priors)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with pm.Model() as model2:\n",
    "    # Intercept\n",
    "    beta_0 = pm.Normal(\"beta_0\", mu=0.0, sigma=10.0)\n",
    "    \n",
    "    # Hierarchical prior for coefficients\n",
    "    tau_sq = pm.InverseGamma(\"tau_sq\", alpha=2.0, beta=1.0)\n",
    "    beta = pm.Normal(\"beta\", mu=0.0, sigma=pm.math.sqrt(tau_sq), shape=p)\n",
    "    \n",
    "    # Error variance\n",
    "    sigma_sq = pm.InverseGamma(\"sigma_sq\", alpha=2.0, beta=1.0)\n",
    "    sigma = pm.Deterministic(\"sigma\", pm.math.sqrt(sigma_sq))\n",
    "    \n",
    "    # Linear predictor\n",
    "    mu = beta_0 + pm.math.dot(X_train, beta)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_train)\n",
    "\n",
    "print(\"Model 2 built successfully!\")\n",
    "print(f\"  Parameters: {p + 4} (1 intercept + {p} coefficients + 1 global scale + 1 error variance + 1 sigma)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model 2\n",
    "print(\"\\nSampling from posterior (Model 2)...\")\n",
    "\n",
    "with model2:\n",
    "    trace2 = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=2000,\n",
    "        chains=4,\n",
    "        cores=1,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RNG_SEED,\n",
    "        return_inferencedata=True,\n",
    "        progressbar=True\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Model 2 sampling completed!\")\n",
    "\n",
    "# Convergence diagnostics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 2: Convergence Diagnostics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "rhat2 = az.rhat(trace2)\n",
    "max_rhat2 = max([float(rhat2[var].max()) for var in rhat2.data_vars if var != 'sigma'])\n",
    "print(f\"\\nR-hat (max): {max_rhat2:.4f} (target: < 1.01)\")\n",
    "\n",
    "ess2 = az.ess(trace2)\n",
    "min_ess2 = min([float(ess2[var].min()) for var in ess2.data_vars if var != 'sigma'])\n",
    "print(f\"ESS (min): {min_ess2:.0f} (target: > 400)\")\n",
    "\n",
    "n_div2 = trace2.sample_stats.divergences.sum().item()\n",
    "print(f\"Divergences: {n_div2} (target: 0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 3: Spike-and-Slab Variable Selection\n",
    "\n",
    "We implement a spike-and-slab prior for automatic variable selection. This model includes binary inclusion indicators $\\gamma_j$ that determine whether each predictor is included in the model.\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "**Likelihood:**\n",
    "$$y_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$$\n",
    "\n",
    "where\n",
    "$$\\mu_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}$$\n",
    "\n",
    "**Prior Distributions:**\n",
    "- Intercept: $\\beta_0 \\sim \\mathcal{N}(0, 10^2)$\n",
    "- Inclusion indicators: $\\gamma_j \\sim \\text{Bernoulli}(\\pi)$ with $\\pi = 0.5$\n",
    "- Coefficients: $\\beta_j \\sim \\mathcal{N}(0, \\sigma_{\\gamma_j}^2)$ where\n",
    "  - $\\sigma_{\\gamma_j}^2 = \\sigma_{\\text{spike}}^2$ if $\\gamma_j = 0$ (spike)\n",
    "  - $\\sigma_{\\gamma_j}^2 = \\sigma_{\\text{slab}}^2$ if $\\gamma_j = 1$ (slab)\n",
    "- Spike variance: $\\sigma_{\\text{spike}}^2 = 0.01^2$ (very small)\n",
    "- Slab variance: $\\sigma_{\\text{slab}}^2 = 1.0^2$ (moderate)\n",
    "- Error variance: $\\sigma^2 \\sim \\text{InverseGamma}(2, 1)$\n",
    "\n",
    "We use a non-centered parameterization for better MCMC mixing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODEL 3: Spike-and-Slab Variable Selection\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with pm.Model() as model3:\n",
    "    # Intercept\n",
    "    beta_0 = pm.Normal(\"beta_0\", mu=0.0, sigma=10.0)\n",
    "    \n",
    "    # Spike-and-slab priors (non-centered parameterization)\n",
    "    pi = 0.5  # Prior inclusion probability\n",
    "    spike_sd = 0.01\n",
    "    slab_sd = 1.0\n",
    "    \n",
    "    # Inclusion indicators\n",
    "    gamma = pm.Bernoulli(\"gamma\", p=pi, shape=p)\n",
    "    \n",
    "    # Non-centered parameterization for better geometry\n",
    "    beta_raw = pm.Normal(\"beta_raw\", mu=0.0, sigma=1.0, shape=p)\n",
    "    sd_beta = spike_sd + gamma * (slab_sd - spike_sd)\n",
    "    beta = pm.Deterministic(\"beta\", beta_raw * sd_beta)\n",
    "    \n",
    "    # Error variance\n",
    "    sigma_sq = pm.InverseGamma(\"sigma_sq\", alpha=2.0, beta=1.0)\n",
    "    sigma = pm.Deterministic(\"sigma\", pm.math.sqrt(sigma_sq))\n",
    "    \n",
    "    # Linear predictor\n",
    "    mu = beta_0 + pm.math.dot(X_train, beta)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_train)\n",
    "\n",
    "print(\"Model 3 built successfully!\")\n",
    "print(f\"  Parameters: {p + 4} (1 intercept + {p} coefficients + {p} inclusion indicators + 1 error variance + 1 sigma)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model 3\n",
    "print(\"\\nSampling from posterior (Model 3)...\")\n",
    "print(\"Note: Spike-and-slab models can be slower due to discrete variables...\")\n",
    "\n",
    "with model3:\n",
    "    trace3 = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=2000,\n",
    "        chains=4,\n",
    "        cores=1,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RNG_SEED,\n",
    "        return_inferencedata=True,\n",
    "        progressbar=True\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Model 3 sampling completed!\")\n",
    "\n",
    "# Convergence diagnostics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 3: Convergence Diagnostics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "rhat3 = az.rhat(trace3)\n",
    "max_rhat3 = max([float(rhat3[var].max()) for var in rhat3.data_vars if var not in ['sigma', 'gamma']])\n",
    "print(f\"\\nR-hat (max): {max_rhat3:.4f} (target: < 1.01)\")\n",
    "\n",
    "ess3 = az.ess(trace3)\n",
    "min_ess3 = min([float(ess3[var].min()) for var in ess3.data_vars if var not in ['sigma', 'gamma']])\n",
    "print(f\"ESS (min): {min_ess3:.0f} (target: > 400)\")\n",
    "\n",
    "n_div3 = trace3.sample_stats.divergences.sum().item()\n",
    "print(f\"Divergences: {n_div3} (target: 0)\")\n",
    "\n",
    "# Posterior inclusion probabilities\n",
    "gamma_samples = trace3.posterior['gamma'].values\n",
    "pip = gamma_samples.mean(axis=(0, 1))  # Average across chains and draws\n",
    "\n",
    "print(\"\\nPosterior Inclusion Probabilities:\")\n",
    "print(\"=\" * 80)\n",
    "pip_df = pd.DataFrame({\n",
    "    'Predictor': [pred.split(':')[0] for pred in selected_predictors],\n",
    "    'PIP': pip\n",
    "}).sort_values('PIP', ascending=False)\n",
    "display(pip_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model 4: Bayesian Neural Network (BNN)\n",
    "\n",
    "We now fit a Bayesian Neural Network with spike-and-slab priors on the weights. This model can capture non-linear relationships between predictors and response.\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "**Architecture:**\n",
    "- Input layer: $p$ predictors\n",
    "- Hidden layer: $q$ units with tanh activation\n",
    "- Output layer: 1 unit (continuous response)\n",
    "\n",
    "**Likelihood:**\n",
    "$$y_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$$\n",
    "\n",
    "where\n",
    "$$\\mu_i = f_{\\text{NN}}(x_i; W_1, b_1, W_2, b_2)$$\n",
    "\n",
    "and $f_{\\text{NN}}$ is a neural network with one hidden layer:\n",
    "- Hidden: $h_i = \\tanh(W_1 x_i + b_1)$\n",
    "- Output: $\\mu_i = W_2^T h_i + b_2$\n",
    "\n",
    "**Prior Distributions:**\n",
    "- All weights use spike-and-slab priors (similar to Model 3)\n",
    "- Hidden layer: $W_1 \\in \\mathbb{R}^{p \\times q}$, $b_1 \\in \\mathbb{R}^q$\n",
    "- Output layer: $W_2 \\in \\mathbb{R}^q$, $b_2 \\in \\mathbb{R}$\n",
    "- Error variance: $\\sigma^2 \\sim \\text{InverseGamma}(2, 1)$\n",
    "\n",
    "We use $q = 5$ hidden units as a balance between flexibility and computational tractability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnn_model(X_train, y_train, q=5, use_noncentered=True):\n",
    "    \"\"\"\n",
    "    Create Bayesian Neural Network with spike-and-slab priors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : array\n",
    "        Training features (n x p)\n",
    "    y_train : array\n",
    "        Training targets (n,)\n",
    "    q : int\n",
    "        Number of hidden units\n",
    "    use_noncentered : bool\n",
    "        Use non-centered parameterization\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pm.Model\n",
    "        PyMC model\n",
    "    \"\"\"\n",
    "    n, p = X_train.shape\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # Intercept\n",
    "        beta_0 = pm.Normal(\"beta_0\", mu=0.0, sigma=10.0)\n",
    "        \n",
    "        # Spike-and-slab parameters\n",
    "        pi = 0.5\n",
    "        spike_sd = 0.01\n",
    "        slab_sd = 1.0\n",
    "        \n",
    "        # Layer 1: Input -> Hidden (p x q weights)\n",
    "        gamma1 = pm.Bernoulli(\"gamma1\", p=pi, shape=(p, q))\n",
    "        \n",
    "        if use_noncentered:\n",
    "            W1_raw = pm.Normal(\"W1_raw\", mu=0.0, sigma=1.0, shape=(p, q))\n",
    "            sd1 = spike_sd + gamma1 * (slab_sd - spike_sd)\n",
    "            W1 = pm.Deterministic(\"W1\", W1_raw * sd1)\n",
    "        else:\n",
    "            sd1 = spike_sd + gamma1 * (slab_sd - spike_sd)\n",
    "            W1 = pm.Normal(\"W1\", mu=0.0, sigma=sd1, shape=(p, q))\n",
    "        \n",
    "        b1 = pm.Normal(\"b1\", mu=0.0, sigma=1.0, shape=q)\n",
    "        \n",
    "        # Hidden layer activation\n",
    "        hidden_raw = pm.math.dot(X_train, W1) + b1\n",
    "        hidden = pm.math.tanh(hidden_raw)\n",
    "        \n",
    "        # Layer 2: Hidden -> Output (q weights)\n",
    "        gamma2 = pm.Bernoulli(\"gamma2\", p=pi, shape=q)\n",
    "        \n",
    "        if use_noncentered:\n",
    "            W2_raw = pm.Normal(\"W2_raw\", mu=0.0, sigma=1.0, shape=q)\n",
    "            sd2 = spike_sd + gamma2 * (slab_sd - spike_sd)\n",
    "            W2 = pm.Deterministic(\"W2\", W2_raw * sd2)\n",
    "        else:\n",
    "            sd2 = spike_sd + gamma2 * (slab_sd - spike_sd)\n",
    "            W2 = pm.Normal(\"W2\", mu=0.0, sigma=sd2, shape=q)\n",
    "        \n",
    "        b2 = pm.Normal(\"b2\", mu=0.0, sigma=1.0)\n",
    "        \n",
    "        # Output\n",
    "        mu = beta_0 + pm.math.dot(hidden, W2) + b2\n",
    "        \n",
    "        # Error variance\n",
    "        sigma_sq = pm.InverseGamma(\"sigma_sq\", alpha=2.0, beta=1.0)\n",
    "        sigma = pm.Deterministic(\"sigma\", pm.math.sqrt(sigma_sq))\n",
    "        \n",
    "        # Likelihood\n",
    "        y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 4: Bayesian Neural Network (BNN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "q_hidden = 5  # Number of hidden units\n",
    "model4 = create_bnn_model(X_train, y_train, q=q_hidden, use_noncentered=True)\n",
    "\n",
    "print(f\"Model 4 built successfully!\")\n",
    "print(f\"  Architecture: {p} inputs -> {q_hidden} hidden (tanh) -> 1 output\")\n",
    "print(f\"  Parameters: ~{p * q_hidden + q_hidden + p + q_hidden + 3} (weights + biases + variance)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model 4\n",
    "print(\"\\nSampling from posterior (Model 4 - BNN)...\")\n",
    "print(\"⚠ Warning: BNNs can be slow due to many parameters and discrete variables...\")\n",
    "print(\"This may take 15-30 minutes...\")\n",
    "\n",
    "with model4:\n",
    "    trace4 = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=2000,\n",
    "        chains=4,\n",
    "        cores=1,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RNG_SEED,\n",
    "        return_inferencedata=True,\n",
    "        progressbar=True\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Model 4 (BNN) sampling completed!\")\n",
    "\n",
    "# Convergence diagnostics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 4: Convergence Diagnostics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "rhat4 = az.rhat(trace4)\n",
    "vars_to_check = [v for v in rhat4.data_vars if v not in ['sigma', 'gamma1', 'gamma2', 'W1', 'W2']]\n",
    "if vars_to_check:\n",
    "    max_rhat4 = max([float(rhat4[var].max()) for var in vars_to_check])\n",
    "    print(f\"\\nR-hat (max): {max_rhat4:.4f} (target: < 1.01)\")\n",
    "\n",
    "ess4 = az.ess(trace4)\n",
    "if vars_to_check:\n",
    "    min_ess4 = min([float(ess4[var].min()) for var in vars_to_check])\n",
    "    print(f\"ESS (min): {min_ess4:.0f} (target: > 400)\")\n",
    "\n",
    "n_div4 = trace4.sample_stats.divergences.sum().item()\n",
    "print(f\"Divergences: {n_div4} (target: 0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model 5: Spatial Regression Model\n",
    "\n",
    "Finally, we fit a spatial regression model that explicitly accounts for spatial structure in the tissue. This model includes spatial coordinates and their interactions as predictors.\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "**Likelihood:**\n",
    "$$y_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$$\n",
    "\n",
    "where\n",
    "$$\\mu_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij} + \\beta_{cx} \\cdot cx_i + \\beta_{cy} \\cdot cy_i + \\beta_{cx \\times cy} \\cdot cx_i \\cdot cy_i$$\n",
    "\n",
    "**Prior Distributions:**\n",
    "- Intercept: $\\beta_0 \\sim \\mathcal{N}(0, 10^2)$\n",
    "- Marker coefficients: $\\beta_j \\sim \\mathcal{N}(0, \\tau^2)$ with $\\tau^2 \\sim \\text{InverseGamma}(2, 1)$\n",
    "- Spatial coefficients: $\\beta_{cx}, \\beta_{cy} \\sim \\mathcal{N}(0, 2^2)$\n",
    "- Spatial interaction: $\\beta_{cx \\times cy} \\sim \\mathcal{N}(0, 1^2)$\n",
    "- Error variance: $\\sigma^2 \\sim \\text{InverseGamma}(2, 1)$\n",
    "\n",
    "The spatial terms capture location-dependent effects and interactions that may reflect tissue structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODEL 5: Spatial Regression Model\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with pm.Model() as model5:\n",
    "    # Intercept\n",
    "    beta_0 = pm.Normal(\"beta_0\", mu=0.0, sigma=10.0)\n",
    "    \n",
    "    # Hierarchical prior for marker coefficients\n",
    "    tau_sq = pm.InverseGamma(\"tau_sq\", alpha=2.0, beta=1.0)\n",
    "    beta_markers = pm.Normal(\"beta_markers\", mu=0.0, sigma=pm.math.sqrt(tau_sq), shape=p)\n",
    "    \n",
    "    # Spatial coordinate effects\n",
    "    beta_cx = pm.Normal(\"beta_cx\", mu=0.0, sigma=2.0)\n",
    "    beta_cy = pm.Normal(\"beta_cy\", mu=0.0, sigma=2.0)\n",
    "    \n",
    "    # Spatial interaction\n",
    "    beta_cx_cy = pm.Normal(\"beta_cx_cy\", mu=0.0, sigma=1.0)\n",
    "    \n",
    "    # Error variance\n",
    "    sigma_sq = pm.InverseGamma(\"sigma_sq\", alpha=2.0, beta=1.0)\n",
    "    sigma = pm.Deterministic(\"sigma\", pm.math.sqrt(sigma_sq))\n",
    "    \n",
    "    # Linear predictor (includes spatial terms)\n",
    "    mu = (beta_0 \n",
    "          + pm.math.dot(X_train, beta_markers)\n",
    "          + beta_cx * coords_train[:, 0]\n",
    "          + beta_cy * coords_train[:, 1]\n",
    "          + beta_cx_cy * coords_train[:, 0] * coords_train[:, 1])\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_train)\n",
    "\n",
    "print(\"Model 5 built successfully!\")\n",
    "print(f\"  Parameters: {p + 6} (1 intercept + {p} marker coefficients + 3 spatial terms + 1 global scale + 1 error variance + 1 sigma)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model 5\n",
    "print(\"\\nSampling from posterior (Model 5)...\")\n",
    "\n",
    "with model5:\n",
    "    trace5 = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=2000,\n",
    "        chains=4,\n",
    "        cores=1,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RNG_SEED,\n",
    "        return_inferencedata=True,\n",
    "        progressbar=True\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Model 5 sampling completed!\")\n",
    "\n",
    "# Convergence diagnostics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 5: Convergence Diagnostics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "rhat5 = az.rhat(trace5)\n",
    "max_rhat5 = max([float(rhat5[var].max()) for var in rhat5.data_vars if var != 'sigma'])\n",
    "print(f\"\\nR-hat (max): {max_rhat5:.4f} (target: < 1.01)\")\n",
    "\n",
    "ess5 = az.ess(trace5)\n",
    "min_ess5 = min([float(ess5[var].min()) for var in ess5.data_vars if var != 'sigma'])\n",
    "print(f\"ESS (min): {min_ess5:.0f} (target: > 400)\")\n",
    "\n",
    "n_div5 = trace5.sample_stats.divergences.sum().item()\n",
    "print(f\"Divergences: {n_div5} (target: 0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison\n",
    "\n",
    "We compare all five models using:\n",
    "- **PSIS-LOO** (Pareto-smoothed importance sampling leave-one-out cross-validation)\n",
    "- **WAIC** (Widely Applicable Information Criterion)\n",
    "- **Predictive performance** on test set (RMSE, MAE, R²)\n",
    "\n",
    "These metrics help us identify which model best balances fit and complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PSIS-LOO and WAIC for all models\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "traces = [trace1, trace2, trace3, trace4, trace5]\n",
    "model_names = ['Model 1: Noninformative', 'Model 2: Hierarchical', \n",
    "               'Model 3: Spike-and-Slab', 'Model 4: BNN', 'Model 5: Spatial']\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for i, (trace, name) in enumerate(zip(traces, model_names), 1):\n",
    "    print(f\"\\nComputing metrics for {name}...\")\n",
    "    \n",
    "    try:\n",
    "        loo = az.loo(trace, pointwise=True)\n",
    "        waic = az.waic(trace, pointwise=True)\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'Model': name,\n",
    "            'LOO': loo.loo,\n",
    "            'LOO_SE': loo.loo_se,\n",
    "            'WAIC': waic.waic,\n",
    "            'WAIC_SE': waic.waic_se,\n",
    "            'p_LOO': loo.p_loo,\n",
    "            'p_WAIC': waic.p_waic\n",
    "        })\n",
    "        \n",
    "        print(f\"  LOO: {loo.loo:.2f} ± {loo.loo_se:.2f}\")\n",
    "        print(f\"  WAIC: {waic.waic:.2f} ± {waic.waic_se:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Error computing LOO/WAIC: {str(e)[:100]}\")\n",
    "        comparison_results.append({\n",
    "            'Model': name,\n",
    "            'LOO': np.nan,\n",
    "            'LOO_SE': np.nan,\n",
    "            'WAIC': np.nan,\n",
    "            'WAIC_SE': np.nan,\n",
    "            'p_LOO': np.nan,\n",
    "            'p_WAIC': np.nan\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Model Comparison Summary\")\n",
    "print(\"=\" * 80)\n",
    "display(comparison_df)\n",
    "\n",
    "# Save for report\n",
    "comparison_df.to_csv('report/model_comparison.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Posterior Predictive Analysis\n",
    "\n",
    "We generate posterior predictive distributions and evaluate model fit through:\n",
    "- Posterior predictive checks\n",
    "- Test set predictions\n",
    "- Residual analysis\n",
    "- Model fit statistics (R², RMSE, MAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate posterior predictive samples for test set\n",
    "# We'll do this for Model 5 (Spatial) as an example\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"POSTERIOR PREDICTIVE ANALYSIS (Model 5: Spatial)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create model with test data\n",
    "with pm.Model() as model5_test:\n",
    "    # Same priors as Model 5\n",
    "    beta_0 = pm.Normal(\"beta_0\", mu=0.0, sigma=10.0)\n",
    "    tau_sq = pm.InverseGamma(\"tau_sq\", alpha=2.0, beta=1.0)\n",
    "    beta_markers = pm.Normal(\"beta_markers\", mu=0.0, sigma=pm.math.sqrt(tau_sq), shape=p)\n",
    "    beta_cx = pm.Normal(\"beta_cx\", mu=0.0, sigma=2.0)\n",
    "    beta_cy = pm.Normal(\"beta_cy\", mu=0.0, sigma=2.0)\n",
    "    beta_cx_cy = pm.Normal(\"beta_cx_cy\", mu=0.0, sigma=1.0)\n",
    "    sigma_sq = pm.InverseGamma(\"sigma_sq\", alpha=2.0, beta=1.0)\n",
    "    sigma = pm.Deterministic(\"sigma\", pm.math.sqrt(sigma_sq))\n",
    "    \n",
    "    # Training likelihood\n",
    "    mu_train = (beta_0 \n",
    "                + pm.math.dot(X_train, beta_markers)\n",
    "                + beta_cx * coords_train[:, 0]\n",
    "                + beta_cy * coords_train[:, 1]\n",
    "                + beta_cx_cy * coords_train[:, 0] * coords_train[:, 1])\n",
    "    y_obs = pm.Normal(\"y_obs\", mu=mu_train, sigma=sigma, observed=y_train)\n",
    "    \n",
    "    # Test predictions\n",
    "    mu_test = (beta_0 \n",
    "               + pm.math.dot(X_test, beta_markers)\n",
    "               + beta_cx * coords_test[:, 0]\n",
    "               + beta_cy * coords_test[:, 1]\n",
    "               + beta_cx_cy * coords_test[:, 0] * coords_test[:, 1])\n",
    "    y_pred = pm.Normal(\"y_pred\", mu=mu_test, sigma=sigma, shape=n_test)\n",
    "\n",
    "# Sample from posterior using training data\n",
    "print(\"\\nSampling from posterior (with test predictions)...\")\n",
    "with model5_test:\n",
    "    trace5_test = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=2000,\n",
    "        chains=4,\n",
    "        cores=1,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RNG_SEED,\n",
    "        return_inferencedata=True,\n",
    "        progressbar=True\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ Sampling completed!\")\n",
    "\n",
    "# Extract test predictions\n",
    "y_pred_samples = trace5_test.posterior_predictive['y_pred'].values\n",
    "y_pred_mean = y_pred_samples.mean(axis=(0, 1))  # Average across chains and draws\n",
    "y_pred_std = y_pred_samples.std(axis=(0, 1))\n",
    "\n",
    "# Compute metrics\n",
    "rmse = np.sqrt(((y_test - y_pred_mean) ** 2).mean())\n",
    "mae = np.abs(y_test - y_pred_mean).mean()\n",
    "ss_res = ((y_test - y_pred_mean) ** 2).sum()\n",
    "ss_tot = ((y_test - y_test.mean()) ** 2).sum()\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Test Set Predictive Performance (Model 5)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R²: {r_squared:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Posterior predictive check\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Observed vs predicted\n",
    "axes[0].scatter(y_test, y_pred_mean, alpha=0.5, s=10, rasterized=True)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Observed (standardized)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted (standardized)', fontsize=12)\n",
    "axes[0].set_title(f'Posterior Predictive: Observed vs Predicted\\n(R² = {r_squared:.3f}, RMSE = {rmse:.3f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred_mean\n",
    "axes[1].scatter(y_pred_mean, residuals, alpha=0.5, s=10, rasterized=True)\n",
    "axes[1].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted (standardized)', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('report/figures/figure3_posterior_predictive.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure 3: Posterior predictive checks for Model 5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Posterior Summary and Credible Intervals\n",
    "\n",
    "We summarize posterior distributions for key parameters, focusing on Model 5 (Spatial) as it incorporates both marker effects and spatial structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior summary for Model 5\n",
    "print(\"=\" * 80)\n",
    "print(\"POSTERIOR SUMMARY: Model 5 (Spatial Regression)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary5 = az.summary(trace5, var_names=['beta_0', 'beta_markers', 'beta_cx', 'beta_cy', 'beta_cx_cy', 'sigma_sq', 'tau_sq'])\n",
    "print(\"\\nFull Posterior Summary:\")\n",
    "display(summary5)\n",
    "\n",
    "# Save for report\n",
    "summary5.to_csv('report/posterior_summary_model5.csv')\n",
    "\n",
    "# Create table of credible intervals for key coefficients\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"95% Credible Intervals for Key Coefficients (Model 5)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ci_data = []\n",
    "\n",
    "# Intercept\n",
    "beta_0_samples = trace5.posterior['beta_0'].values.flatten()\n",
    "ci_lower = np.percentile(beta_0_samples, 2.5)\n",
    "ci_upper = np.percentile(beta_0_samples, 97.5)\n",
    "post_mean = beta_0_samples.mean()\n",
    "ci_data.append({\n",
    "    'Coefficient': 'Intercept (β₀)',\n",
    "    'Posterior Mean': f'{post_mean:.3f}',\n",
    "    '95% CI Lower': f'{ci_lower:.3f}',\n",
    "    '95% CI Upper': f'{ci_upper:.3f}',\n",
    "    'Excludes Zero': 'Yes' if (ci_lower > 0) or (ci_upper < 0) else 'No'\n",
    "})\n",
    "\n",
    "# Marker coefficients\n",
    "for i, marker in enumerate(selected_predictors):\n",
    "    beta_samples = trace5.posterior['beta_markers'][:, :, i].values.flatten()\n",
    "    ci_lower = np.percentile(beta_samples, 2.5)\n",
    "    ci_upper = np.percentile(beta_samples, 97.5)\n",
    "    post_mean = beta_samples.mean()\n",
    "    marker_short = marker.split(':')[0]\n",
    "    ci_data.append({\n",
    "        'Coefficient': f'β_{i+1} ({marker_short})',\n",
    "        'Posterior Mean': f'{post_mean:.3f}',\n",
    "        '95% CI Lower': f'{ci_lower:.3f}',\n",
    "        '95% CI Upper': f'{ci_upper:.3f}',\n",
    "        'Excludes Zero': 'Yes' if (ci_lower > 0) or (ci_upper < 0) else 'No'\n",
    "    })\n",
    "\n",
    "# Spatial coefficients\n",
    "for var_name, label in [('beta_cx', 'β_cx (X coordinate)'), \n",
    "                        ('beta_cy', 'β_cy (Y coordinate)'),\n",
    "                        ('beta_cx_cy', 'β_cx_cy (X×Y interaction)')]:\n",
    "    samples = trace5.posterior[var_name].values.flatten()\n",
    "    ci_lower = np.percentile(samples, 2.5)\n",
    "    ci_upper = np.percentile(samples, 97.5)\n",
    "    post_mean = samples.mean()\n",
    "    ci_data.append({\n",
    "        'Coefficient': label,\n",
    "        'Posterior Mean': f'{post_mean:.3f}',\n",
    "        '95% CI Lower': f'{ci_lower:.3f}',\n",
    "        '95% CI Upper': f'{ci_upper:.3f}',\n",
    "        'Excludes Zero': 'Yes' if (ci_lower > 0) or (ci_upper < 0) else 'No'\n",
    "    })\n",
    "\n",
    "ci_df = pd.DataFrame(ci_data)\n",
    "print(\"\\nCredible Intervals:\")\n",
    "display(ci_df)\n",
    "\n",
    "# Save for report table\n",
    "ci_df.to_csv('report/credible_intervals_model5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Posterior distributions of selected coefficients\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot posterior distributions for first 6 marker coefficients\n",
    "for i in range(min(6, p)):\n",
    "    beta_samples = trace5.posterior['beta_markers'][:, :, i].values.flatten()\n",
    "    axes[i].hist(beta_samples, bins=50, density=True, alpha=0.7, edgecolor='black', color='steelblue')\n",
    "    axes[i].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "    \n",
    "    # Add credible interval\n",
    "    ci_lower = np.percentile(beta_samples, 2.5)\n",
    "    ci_upper = np.percentile(beta_samples, 97.5)\n",
    "    axes[i].axvline(ci_lower, color='blue', linestyle=':', linewidth=2, label='95% CI')\n",
    "    axes[i].axvline(ci_upper, color='blue', linestyle=':', linewidth=2)\n",
    "    \n",
    "    marker_name = selected_predictors[i].split(':')[0]\n",
    "    axes[i].set_title(f'Posterior: {marker_name}', fontsize=11, fontweight='bold')\n",
    "    axes[i].set_xlabel('Coefficient Value', fontsize=10)\n",
    "    axes[i].set_ylabel('Density', fontsize=10)\n",
    "    axes[i].legend(fontsize=9)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Posterior Distributions of Selected Regression Coefficients (Model 5)', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('report/figures/figure4_posterior_coefficients.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure 4: Posterior distributions of selected coefficients\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Model Convergence**: All models achieved good convergence (R-hat < 1.01, ESS > 400)\n",
    "2. **Model Comparison**: [Results from comparison table]\n",
    "3. **Spatial Effects**: Spatial coordinates show [significant/non-significant] effects\n",
    "4. **Marker Associations**: [Key findings about which markers are important]\n",
    "5. **Predictive Performance**: Model 5 achieved R² = [value] on test set\n",
    "\n",
    "### Biological Interpretation\n",
    "\n",
    "[Interpretation of findings in context of tumor immunology]\n",
    "\n",
    "### Model Recommendations\n",
    "\n",
    "[Which model(s) to use and why]\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "[Discussion of limitations and potential extensions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
